---
layout: post
title: "Recurrent Neural Networks 01"
date: 2025-04-14
categories: sequence-models
mathjax: true
tags: [sequence-models, deep-learning]
---

## Sequence Tasks in the Real World

Sequence models are designed to handle data where the order of elements matters. Here are some real-world tasks involving sequential data:

| Task                         | Input (x)                                        | Output (y)                                       |
|------------------------------|--------------------------------------------------|--------------------------------------------------|
| Speech transcription         | Audio waveform                                       | Text transcript                                  |
| Melody generation            | Short seed melody or empty input or texture prompts  | Music Snippets                        |
| Sentiment analysis           | "I didn‚Äôt enjoy the film at all."               | Negative sentiment (e.g., ‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ)                 |
| Genomic pattern detection    | ACTGCTAGCGTTAGCTGA                               | Highlighted segments correspond to protein                    |
| Language translation         | "Shall we go for a walk?"                      | "Êàë‰ª¨ÂéªÊï£Ê≠•Â•ΩÂêóÔºü"                   |
| Entity tagging               | "Yesterday, Tom met Eliza in Paris."            | Tom ‚Üí Person, Eliza ‚Üí Person, Paris ‚Üí Location  |
| Time series forecasting      | Historical temperature readings                 | Predicted future temperature                     |

From the table above, we can see that sequence tasks fall into different categories based on input and output formats:

- **Sequence-to-sequence**: Both input and output are sequences.  
  Examples:  
  - *Speech transcription*: audio ‚Üí text  
  - *Language translation*: sentence ‚Üí translated sentence  
  - *Melody generation*: short prompt ‚Üí music snippet  
  - *Time series forecasting*: past values ‚Üí future values  
  These tasks may have input and output of different lengths.

- **Sequence-to-label**: Input is a sequence, output is a single or aligned label.  
  Examples:  
  - *Sentiment analysis*: sentence ‚Üí sentiment  
  - *Genomic pattern detection*: DNA ‚Üí functional region  
  - *Entity tagging*: sentence ‚Üí tag sequence (same length as input)

Understanding input-output structure helps in choosing the right model (e.g., many-to-one vs many-to-many).


## Notation in Sequence Models

To work with sequence data in models like RNNs, it's important to define clear notation. Here's a typical NLP example:

**Input sequence (x):**  
*"Sherlock Holmes and John Watson lived at 221B Baker Street in London."*

We notate (tokenize) the sentence word by word:  
$$
x^{\langle 1 \rangle} = \text{"Sherlock"},\quad x^{\langle 2 \rangle} = \text{"Holmes"},\quad \dots,\quad x^{\langle t \rangle},\quad \dots,\quad x^{\langle 12 \rangle} = \text{"London"}
$$

We use \\(T_x = 12\\) to denote the length of the input sequence.  

**Output sequence (y):**  
Suppose the task is **named entity recognition (NER)**, where each word gets a binary label:  
$$
y^{\langle 1 \rangle} = 1, y^{\langle 2 \rangle} = 1, \quad \dots, \quad y^{\langle t \rangle},\quad \dots,\quad y^{\langle 12 \rangle} = 0
$$

Here the label \\(y = 1\\) means the corresponding word is a part of a name, while \\(y = 0\\) means the corresponding word is **not** a part of a name. The output length \\(T_y = 12\\) in this case is the same as \\(T_x\\), because we're predicting one label per token. But in general, \\(T_x\\) and \\(T_y\\) could be different for other tasks.  

For the **i-th** training example:
  - Here, one example represents one sentence. The whole dataset may contain a lot of sentences.
  - \\(X^{(i)\langle t \rangle}\\): the **t-th** word in the **i-th** sentence (example) in the dataset.
  - \\(Y^{(i)\langle t \rangle}\\): the **t-th** label in the **i-th** sentence (example) in the dataset.
  - \\(T_x^{(i)}\\), \\(T_y^{(i)}\\): input/output lengths for example **i**.


## Representing Words: One-Hot Encoding

To process natural language with sequence models, we first need to convert words into numerical representations. We do it through two steps.  

1. **Vocabulary construction**

    | Index  | Word        |
    |--------|-------------|
    | 1      | a           |
    | 2      | aaron       |
    | ...    | ...         |
    | 367    | at          |
    | ...    | ...         |
    | 4076   | Holmes      |
    | ...    | ...         |
    | 7235   | Watson      |
    | ...    | ...         |
    | ...    | ...         |
    | 10,000 | `<UNK>`     |

    > Note: Words not in the vocabulary are mapped to `<UNK>` (unknown token).

  Assume vocabulary size \\( |V| = 10000 \\).

2. **One-hot encoding**

    Each word \\( x^{\langle t \rangle} \\) is represented as a one-hot vector of **length 10,000**:
    - A vector with all zeros except a 1 at the index corresponding to the word
    - For example:  
        - "Holmes" ‚Üí a vector with 1 at position 4076, 0s elsewhere  
          $$
          x^{\langle 2 \rangle} = \text{one-hot}("Holmes") =
          [0,\, 0,\, \dots,\, 0,\, \underset{4076}{1},\, 0,\, \dots,\, 0] \in \mathbb{R}^{10000}
          $$
        - "Watson" ‚Üí 1 at position 7235, 0s elsewhere  
          $$
          x^{\langle 5 \rangle} = \text{one-hot}("Watson") =
          [0,\, 0,\, \dots,\, 0,\, \underset{7235}{1},\, 0,\, \dots,\, 0] \in \mathbb{R}^{10000}
          $$
        - "221B" ‚Üí 1 at position 10000, 0s elsewhere ("221B" is not in our vocabulary, so it is mapped to `<UNK>`)
          $$
          x^{\langle 8 \rangle} = \text{one-hot}("221B") =
          [0,\, 0,\, \dots,\, \underset{10000}{1}] \in \mathbb{R}^{10000}
          $$

    üîç One-hot encoding is simple and works well for small vocabularies, but:  
      - It does **not capture semantic similarity** (e.g., "cat" and "dog" are equally distant as "cat" and "carpet").  
      - The vectors are **sparse** and **high-dimensional**.  

    Later we'll explore more expressive word representations like **word embeddings** (e.g., word2vec, GloVe).

    Thus, an entire sentence can be represented as a sequence of one-hot vectors, typically organized into a 2D array (matrix) of shape \\( T \times V \\), where \\( T \\) is the number of tokens in the sentence and \\( V \\) is the vocabulary size. Our task is to find a map \\(f\\) to map from the input \\(x\\) to the target \\(y\\).

## Can we use a standard network?

The following image shows that we apply a fully connected network.  

![fc_net](/assets/img/sequence_models/fc.jpg){: .mx-auto.d-block :}

To let a fully connected (FC) network process sequence data, such as \\( 12 \times 10000 \\), we first need to flatten the data into a single long vector before feeding it into the network. However, FC networks are not suitable for processing sequence data, mainly for the following reasons:
  - The inputs and outputs can have different lengths in other tasks, e.g. translation, but the input and output dimensions of the FC networks are fixed, which means the FC networks can only process sentences of fixed length;
  - Flattening the input sequence into a single vector destroys its structure. The model can no longer distinguish which features belong to which word or time step.
  - The model cannot share features across positions. For example, in the sentence above, the word vectors for "Sherlock" and "lived" are processed using different weights, so the model cannot generalize patterns learned at one position to others. If the word order changes (e.g. "At 221B Baker Street lived Sherlock..."), the model may fail to recognize "Sherlock" as a name. In contrast, a model with shared weights could learn general patterns, such as "Sherlock" usually being a noun and "lived" a verb, regardless of position.

## RNN

The following image illustrates the basic structure of RNNs. Here we assume \\(T_x = T_y\\)

![fc_net](/assets/img/sequence_models/rnn_forward.drawio.svg){: .mx-auto.d-block :}

Each vertical block in the diagram is a **recurrent unit**, and the full structure shows how information flows across time steps in an RNN.

At time step \\( t \\), the RNN takes:
- the **current input** \\( x^{\langle t \rangle} \\)
- the **previous hidden state** \\( a^{\langle t-1 \rangle} \\)

and computes:
- the **new hidden state** \\( a^{\langle t \rangle} \\)
- the **predicted output** \\( \hat{y}^{\langle t \rangle} \\)

Formally, the computation at each time step is:  

$$
a^{\langle t \rangle} = \text{g}(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)
$$

$$
\hat{y}^{\langle t \rangle} = \text{g}(W_{ya} a^{\langle t \rangle} + b_y)
$$

> Note: here we use subscripts for all weight matrices.
> E.g., \\(W_{ax}\\) means this weight matrix needs to multiply an \\(x\\) on the right side,
> and the result is an \\(a\\)

Here the \\(g\\) represents activation. Usually we use \\(\tanh\\) for calculating \\(a\\). For the prediction \\(\hat{y}\\), the activation depends on the task.  

üîÅ Parameter sharing  
All time steps share the **same set of weights**:
  - \\( W_{ax} \\): input to hidden
  - \\( W_{aa} \\): hidden to hidden
  - \\( W_{ya} \\): hidden to output  
This enables the model to **learn generalizable patterns** across positions in the sequence. The hidden state \\( a^{\langle t \rangle} \\) acts as a **memory**, allowing the model to carry information from the past into the future. This is what makes RNNs different from traditional fully connected networks‚Äîthey **understand order and context**.  

We also have simplified notation for RNNs.

$$
a^{\langle t \rangle} = \text{g}(W_{a} [a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_a)
$$

$$
\hat{y}^{\langle t \rangle} = \text{g}(W_{y} a^{\langle t \rangle} + b_y)
$$

where \\(W_{a}\\) is horizontal stack of \\(W_{aa}\\) and \\(W_{ax}\\).

## Limitation of classical RNNs

We may notice that the prediction at time step \\( t \\), namely \\( \hat{y}^{\langle t \rangle} \\), depends only on the current and previous inputs, but **not** on any future inputs.

This is appropriate for tasks like **time series forecasting**, where future information is unknown and must not be used.

However, in many **natural language processing** tasks, such as **part-of-speech tagging** or **NER**, the interpretation of a word often depends on both **preceding** and **following** words. For example, in the sentence:

```
He met with Apple representatives today.
```

To correctly classify "Apple" as an **organization** (rather than a fruit), the model needs to see the word **"representatives"** that comes *after* it. Classical RNNs, which only process input from left to right, are unable to access such future context ‚Äî limiting their ability to make accurate predictions.

This motivates the use of models like **Bidirectional RNNs**, which can incorporate both past and future information when making predictions.
